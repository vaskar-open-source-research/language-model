import argparse
import regex as re
from typing import List
from collections import Counter
import json
import os
from tqdm import tqdm
from src.utils import corpus_to_pre_tokens


def naive_train_bpe(input_path: str, vocab_size: int, special_tokens: List[str]):
    
    with open(input_path, 'r') as fp:
        corpus = fp.read()

    pre_tokens = corpus_to_pre_tokens(corpus)
    pre_token_bytes = [tuple([bytes([c]) for c in s.encode('utf-8')]) for s in pre_tokens]
    vocab = {
        i: i.to_bytes() for i in range(256)
    }
    for special_token in special_tokens:
        vocab[len(vocab)] = special_token.encode('utf-8')

    merged_bytes = []

    while pre_token_bytes and len(vocab) < vocab_size:
        pre_token_bytes_counter = Counter(pre_token_bytes)
        byte_pair_counts = {}
        for token_bytes, value in pre_token_bytes_counter.items():
            for i in range(1, len(token_bytes)):
                byte_pair = (token_bytes[i-1], token_bytes[i])
                if byte_pair not in byte_pair_counts:
                    byte_pair_counts[byte_pair] = 0
                byte_pair_counts[byte_pair] += value
        
        most_freqent_byte_pair = sorted(list(byte_pair_counts.items()), key=lambda x: (x[1], x[0]))[-1][0]
        merged_bytes.append(most_freqent_byte_pair)
        vocab[len(vocab)] = most_freqent_byte_pair[0] + most_freqent_byte_pair[1]

        if len(vocab) == vocab_size:
            break
        # merge the bytes in pre_token_bytes
        new_pre_token_bytes = []
        for i, token_bytes in enumerate(pre_token_bytes):
            new_token_bytes = []
            i = 0
            while i < len(token_bytes):
                if i + 1 < len(token_bytes) and (token_bytes[i], token_bytes[i+1]) == most_freqent_byte_pair:
                    new_token_bytes.append(token_bytes[i] + token_bytes[i+1])
                    i += 2
                else:
                    new_token_bytes.append(token_bytes[i])
                    i += 1
            if len(new_token_bytes) > 1:
                new_pre_token_bytes.append(tuple(new_token_bytes))
        pre_token_bytes = new_pre_token_bytes

    return vocab, merged_bytes


def optmized_train_bpe(input_path: str, vocab_size: int, special_tokens: List[str], verbose: bool = False):
    
    if verbose:
        print(f"Started training...")
    if verbose:
        print(f"Started reading file...")
    with open(input_path, 'r') as fp:
        corpus = fp.read()
    if verbose:
        print(f"Finished reading file.")

    if verbose:
        print(f"Generating pre tokens...")
    pre_tokens = corpus_to_pre_tokens(corpus)
    if verbose:
        print(f"Generated pre tokens.")

    if verbose:
        print(f"Encoding pre tokens...")
    pre_token_bytes = [tuple([bytes([c]) for c in s.encode('utf-8')]) for s in pre_tokens]
    if verbose:
        print(f"Encoded pre tokens.")

    vocab = {
        i: bytes([i]) for i in range(256)
    }
    
    for special_token in special_tokens:
        vocab[len(vocab)] = special_token.encode('utf-8')
    
    merged_bytes = []
    
    if verbose:
        print(f"Counting pre tokens...")
    pre_token_bytes_counter = Counter(pre_token_bytes)
    list_of_pre_token_bytes_counter = list([seq, cnt] for seq, cnt in pre_token_bytes_counter.items())
    if verbose:
        print(f"Counted pre tokens.")

    byte_pair_counts = {}
    byte_pair_indices = {}
    seen_bytes = set()
    if verbose:
        print(f"Generating byte pairs...")
    for k, (token_bytes, value) in tqdm(enumerate(list_of_pre_token_bytes_counter), disable=not verbose):
        for i in range(1, len(token_bytes)):
            byte_pair = (token_bytes[i-1], token_bytes[i])
            if byte_pair not in byte_pair_counts:
                byte_pair_counts[byte_pair] = 0
            byte_pair_counts[byte_pair] += value
            if byte_pair not in byte_pair_indices:
                    byte_pair_indices[byte_pair] = []
            if i < len(token_bytes):
                byte_pair_indices[byte_pair].append(k)
    if verbose:
        print(f"Generated byte pairs...")

    with tqdm(total=vocab_size - len(vocab), desc="Train", disable=not verbose) as pbar:
        while pre_token_bytes and len(vocab) < vocab_size:
            pbar.update(1)
            most_freqent_byte_pair = sorted(list(byte_pair_counts.items()), key=lambda x: (x[1], x[0]))
            # find the first byte pair from the end of the list that is not in vocab
            for i in range(len(most_freqent_byte_pair) - 1, -1, -1):
                if most_freqent_byte_pair[i][0][0] + most_freqent_byte_pair[i][0][1] not in seen_bytes:
                    most_freqent_byte_pair = most_freqent_byte_pair[i][0]
                    break
            merged_bytes.append(most_freqent_byte_pair)
            vocab[len(vocab)] = most_freqent_byte_pair[0] + most_freqent_byte_pair[1]
            seen_bytes.add(most_freqent_byte_pair[0] + most_freqent_byte_pair[1])

            if len(vocab) == vocab_size:
                break
            # merge the bytes in pre_token_bytes
            byte_pair_counts.pop(most_freqent_byte_pair)
            
            for idx in byte_pair_indices[most_freqent_byte_pair]:
                seq, count = list_of_pre_token_bytes_counter[idx]
                new_token_bytes = []
                i = 0
                while i < len(seq):
                    if i + 1 < len(seq) and (seq[i], seq[i+1]) == most_freqent_byte_pair:
                        new_token_bytes.append(seq[i] + seq[i+1])
                        
                        if i - 1 >= 0 and (seq[i-1], seq[i]) in byte_pair_counts:
                            byte_pair_counts[(seq[i-1], seq[i])] -= count
                        
                        if i + 2 < len(seq) and (seq[i+1], seq[i+2]) in byte_pair_counts:
                            byte_pair_counts[(seq[i+1], seq[i+2])] -= count
                        
                        if i - 1 >= 0:
                            left = (seq[i - 1], seq[i] + seq[i+1])
                            if left not in byte_pair_counts:
                                byte_pair_counts[left] = 0
                            if left not in byte_pair_indices:
                                byte_pair_indices[left] = []
                            byte_pair_indices[left].append(idx)
                            byte_pair_counts[left] += count

                        if i + 2 < len(seq):
                            right = (seq[i] + seq[i + 1], seq[i + 2])
                            if right not in byte_pair_counts:
                                byte_pair_counts[right] = 0
                            if right not in byte_pair_indices:
                                byte_pair_indices[right] = []
                            byte_pair_indices[right].append(idx)
                            byte_pair_counts[right] += count
                        i += 2
                    else:
                        new_token_bytes.append(seq[i])
                        i += 1
                
                list_of_pre_token_bytes_counter[idx][0] = new_token_bytes
            
    return vocab, merged_bytes


def main():

    # example call: python cs336_basics/train_bpe.py --input_path cs336_basics/test_files/test_corpus.txt --vocab_size 300 --special_tokens hello
    parser = argparse.ArgumentParser(description='Train bpe tokenizer.')
    parser.add_argument("--input_path", type=str)
    parser.add_argument("--vocab_size", type=int)
    parser.add_argument("--special_tokens", nargs='+', type=str)

    args = parser.parse_args()
        
    vocab, merges = optmized_train_bpe(args.input_path, args.vocab_size, args.special_tokens)

    dir_name = os.path.dirname(args.input_path)
    file_name = args.input_path.split('/')[-1].split('.')[0]

    with open(os.path.join(dir_name, f'{file_name}_vocab.json'), 'w') as fp:
        vocab_dict = {}
        for int_key, byte_value in vocab.items():
            if ''.join([chr(x) for x in list(byte_value)]) in vocab_dict:
                print(f"Duplicate key: {''.join([chr(x) for x in list(byte_value)])}")
                print(f"byte_value: {byte_value}")
                print(f"int_key: {int_key}")
            vocab_dict[''.join([chr(x) for x in list(byte_value)])] = int_key
        json.dump(vocab_dict, fp)
        
    with open(os.path.join(dir_name, f'{file_name}_merges.txt'), 'w') as fp:
        for i, merge in enumerate(merges):
            merge_first = ''.join([chr(x) for x in list(merge[0])]).replace(' ', 'Ġ').replace('\n', 'Ċ')
            merge_second = ''.join([chr(x) for x in list(merge[1])]).replace(' ', 'Ġ').replace('\n', 'Ċ')
            if i == len(merges) - 1:
                fp.write(f"{merge_first} {merge_second}")
            else:
                fp.write(f"{merge_first} {merge_second}\n")
    
if __name__ == "__main__":
    main()






# string
# string -> bytes